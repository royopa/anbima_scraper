# ANBIMA Scraper

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)

> **Web scraper moderno e robusto para captura de dados financeiros da ANBIMA (AssociaÃ§Ã£o Brasileira das Entidades dos Mercados Financeiro e de Capitais)**

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Funcionalidades](#funcionalidades)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [Uso RÃ¡pido](#uso-rÃ¡pido)
- [DocumentaÃ§Ã£o Detalhada](#documentaÃ§Ã£o-detalhada)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [ContribuiÃ§Ã£o](#contribuiÃ§Ã£o)
- [LicenÃ§a](#licenÃ§a)

## ğŸ¯ VisÃ£o Geral

O **ANBIMA Scraper** Ã© uma ferramenta Python moderna e robusta para capturar dados financeiros do site da ANBIMA. O projeto foi completamente refatorado aplicando as melhores prÃ¡ticas de desenvolvimento, incluindo:

- âœ… **Arquitetura orientada a objetos** com classes bem estruturadas
- âœ… **Sistema de logging robusto** com diferentes nÃ­veis e outputs
- âœ… **Tratamento de erros avanÃ§ado** com retry automÃ¡tico
- âœ… **Interface de linha de comando (CLI)** intuitiva
- âœ… **ConfiguraÃ§Ã£o centralizada** e flexÃ­vel
- âœ… **Type hints** para melhor desenvolvimento
- âœ… **Testes automatizados** (estrutura preparada)
- âœ… **DocumentaÃ§Ã£o completa** com docstrings

### ğŸš€ Principais Melhorias

- **Estrutura modular**: CÃ³digo organizado em mÃ³dulos especÃ­ficos
- **ReutilizaÃ§Ã£o**: Classe base `BaseScraper` com funcionalidades comuns
- **Robustez**: Cliente HTTP com retry e rotaÃ§Ã£o de user agents
- **Flexibilidade**: ConfiguraÃ§Ã£o via arquivos e variÃ¡veis de ambiente
- **Monitoramento**: Logs detalhados para troubleshooting
- **Performance**: Download apenas de dados novos

## ğŸ“Š Funcionalidades

### ğŸ¯ Scrapers Implementados

| Scraper | Status | DescriÃ§Ã£o | Dados Capturados |
|---------|--------|-----------|------------------|
| **Indicators** | âœ… Completo | Indicadores financeiros | SELIC, CDI, IPCA, IGP-M, DÃ³lar, Euro, TR, TBF, FDS |
| **IDKA** | âœ… Completo | Ãndice de DuraÃ§Ã£o Constante ANBIMA | Retornos, volatilidade, taxas de juros |
| **IMA Carteiras** | ğŸ”„ Estrutura | Carteiras teÃ³ricas IMA | ComposiÃ§Ã£o de carteiras |
| **IMA Quadro Resumo** | ğŸ”„ Estrutura | Quadro resumo IMA | Resultados diÃ¡rios |
| **Curvas de Juros** | ğŸ”„ Estrutura | Curvas de juros fechamento | Curvas zero-cupom |
| **DebÃªntures** | ğŸ”„ Estrutura | Mercado secundÃ¡rio de debÃªntures | Taxas mÃ©dias |

### ğŸ”§ Funcionalidades AvanÃ§adas

- **VerificaÃ§Ã£o automÃ¡tica**: Detecta dados existentes e baixa apenas o necessÃ¡rio
- **Processamento seguro**: Limpeza automÃ¡tica de dados invÃ¡lidos
- **DeduplicaÃ§Ã£o**: Remove registros duplicados automaticamente
- **OrdenaÃ§Ã£o**: Organiza dados por data de referÃªncia
- **ValidaÃ§Ã£o**: Verifica integridade dos dados baixados
- **Logs estruturados**: Rastreamento completo das operaÃ§Ãµes

## ğŸ› ï¸ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.8 ou superior
- pip (gerenciador de pacotes Python)

### InstalaÃ§Ã£o BÃ¡sica

```bash
# Clone o repositÃ³rio
git clone https://github.com/royopa/anbima_scraper.git
cd anbima_scraper

# Instale as dependÃªncias
pip install -r requirements.txt
```

### InstalaÃ§Ã£o em Modo Desenvolvimento

```bash
# Clone o repositÃ³rio
git clone https://github.com/royopa/anbima_scraper.git
cd anbima_scraper

# Instale em modo desenvolvimento
pip install -e .

# Instale dependÃªncias de desenvolvimento
pip install -r requirements.txt
```

### ConfiguraÃ§Ã£o Inicial

1. **Crie os diretÃ³rios necessÃ¡rios** (criados automaticamente):
   ```bash
   mkdir -p data/raw data/processed logs
   ```

2. **Configure arquivos opcionais**:
   - `user-agents.txt`: Lista de user agents para rotaÃ§Ã£o
   - `ANBIMA.txt`: Arquivo de feriados da ANBIMA

## ğŸš€ Uso RÃ¡pido

### Interface de Linha de Comando (CLI)

```bash
# Executar todos os scrapers
python -m anbima_scraper run-all

# Executar apenas indicadores
python -m anbima_scraper run indicators

# Executar mÃºltiplos scrapers
python -m anbima_scraper run indicators idka

# ForÃ§ar atualizaÃ§Ã£o (mesmo se dados existirem)
python -m anbima_scraper run-all --force

# Verificar status dos scrapers
python -m anbima_scraper status

# Listar scrapers disponÃ­veis
python -m anbima_scraper list
```

### Uso via Python

```python
from anbima_scraper import ANBIMAScraper

# Criar instÃ¢ncia do scraper
scraper = ANBIMAScraper()

# Executar todos os scrapers
results = scraper.run_all()

# Executar scraper especÃ­fico
success = scraper.run_scraper("indicators")

# Verificar status
status = scraper.get_scraper_status()

# Executar mÃºltiplos scrapers
results = scraper.run_multiple(["indicators", "idka"])
```

### Exemplo Completo

```python
#!/usr/bin/env python3
"""
Exemplo completo de uso do ANBIMA Scraper.
"""

from anbima_scraper import ANBIMAScraper

def main():
    # Criar instÃ¢ncia
    scraper = ANBIMAScraper()
    
    # Verificar status atual
    print("Status atual:")
    status = scraper.get_scraper_status()
    for name, last_date in status.items():
        date_str = last_date.strftime("%Y-%m-%d") if last_date else "Sem dados"
        print(f"  {name}: {date_str}")
    
    # Executar scrapers
    print("\nExecutando scrapers...")
    results = scraper.run_all()
    
    # Mostrar resultados
    print("\nResultados:")
    for name, success in results.items():
        status = "âœ“" if success else "âœ—"
        print(f"{status} {name}")

if __name__ == "__main__":
    main()
```

## ğŸ“š DocumentaÃ§Ã£o Detalhada

### ConfiguraÃ§Ã£o

O projeto usa configuraÃ§Ã£o centralizada em `src/anbima_scraper/config/settings.py`:

```python
# URLs da ANBIMA
ANBIMA_URLS = {
    "indicators": "https://www.anbima.com.br/informacoes/indicadores/",
    "idka": "http://www.anbima.com.br/informacoes/idka/IDkA-down.asp",
    # ... outras URLs
}

# ConfiguraÃ§Ãµes de requisiÃ§Ã£o
REQUEST_SETTINGS = {
    "timeout": 30,
    "max_retries": 3,
    "retry_delay": 1,
    # ... outras configuraÃ§Ãµes
}
```

### Estrutura de Dados

#### Indicadores ANBIMA
```csv
data_referencia,data_captura,indice,descricao,valor
2023-12-15,2023-12-15,selic,Taxa SELIC do BC2,13.75
2023-12-15,2023-12-15,cdi,DI-CETIP3,13.65
```

#### IDKA
```csv
dt_referencia,no_indexador,no_indice,nu_indice,ret_dia_perc,ret_mes_perc,ret_ano_perc,ret_12_meses_perc,vol_aa_perc,taxa_juros_aa_perc_compra_d1,taxa_juros_aa_perc_venda_d0
2023-12-15,Prefixado,IDkA-P,1234.56,0.12,2.34,15.67,18.90,8.45,13.50,13.60
```

### Logs

O sistema gera logs estruturados em `logs/anbima_scraper.log`:

```
2023-12-15 10:30:15 [INFO] anbima_scraper.core.anbima_scraper: Starting ANBIMA scraper for all data sources
2023-12-15 10:30:16 [INFO] anbima_scraper.scrapers.indicators: Scraping ANBIMA indicators
2023-12-15 10:30:18 [INFO] anbima_scraper.scrapers.indicators: Indicators data updated successfully
```

## ğŸ—ï¸ Estrutura do Projeto

```
anbima_scraper/
â”œâ”€â”€ src/anbima_scraper/          # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ __init__.py              # Exports principais
â”‚   â”œâ”€â”€ cli.py                   # Interface de linha de comando
â”‚   â”œâ”€â”€ core/                    # Funcionalidades centrais
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ anbima_scraper.py    # Classe principal
â”‚   â”‚   â””â”€â”€ scraper.py           # Classe base
â”‚   â”œâ”€â”€ scrapers/                # Scrapers especÃ­ficos
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ indicators.py        # Indicadores ANBIMA
â”‚   â”‚   â”œâ”€â”€ idka.py             # IDKA
â”‚   â”‚   â”œâ”€â”€ ima.py              # IMA
â”‚   â”‚   â”œâ”€â”€ curves.py           # Curvas de juros
â”‚   â”‚   â””â”€â”€ debentures.py       # DebÃªntures
â”‚   â”œâ”€â”€ utils/                   # UtilitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ http_client.py      # Cliente HTTP
â”‚   â”‚   â”œâ”€â”€ calendar.py         # UtilitÃ¡rios de calendÃ¡rio
â”‚   â”‚   â””â”€â”€ data_processor.py   # Processamento de dados
â”‚   â””â”€â”€ config/                  # ConfiguraÃ§Ãµes
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ settings.py         # ConfiguraÃ§Ãµes centralizadas
â”œâ”€â”€ data/                        # Dados processados
â”‚   â”œâ”€â”€ raw/                     # Dados brutos
â”‚   â””â”€â”€ processed/               # Dados processados
â”œâ”€â”€ tests/                       # Testes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_indicators.py      # Testes dos indicadores
â”œâ”€â”€ examples/                    # Exemplos de uso
â”‚   â””â”€â”€ basic_usage.py          # Exemplo bÃ¡sico
â”œâ”€â”€ logs/                        # Logs do sistema
â”œâ”€â”€ pyproject.toml              # ConfiguraÃ§Ã£o do projeto
â”œâ”€â”€ requirements.txt            # DependÃªncias
â”œâ”€â”€ .pre-commit-config.yaml    # Hooks de pre-commit
â”œâ”€â”€ .gitignore                 # Arquivos ignorados
â””â”€â”€ README.md                  # Este arquivo
```

### Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI Module    â”‚    â”‚  Python API     â”‚    â”‚  Configuration  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  run-all        â”‚    â”‚  ANBIMAScraper  â”‚    â”‚  settings.py    â”‚
â”‚  run            â”‚    â”‚  run_all()      â”‚    â”‚  URLs           â”‚
â”‚  status         â”‚    â”‚  run_scraper()  â”‚    â”‚  Settings       â”‚
â”‚  list           â”‚    â”‚  get_status()   â”‚    â”‚  Logging        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Base Scraper   â”‚
                    â”‚                 â”‚
                    â”‚  BaseScraper    â”‚
                    â”‚  - HTTP Client  â”‚
                    â”‚  - Calendar     â”‚
                    â”‚  - Data Proc    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Indicators     â”‚    â”‚     IDKA        â”‚    â”‚     Others      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  - SELIC        â”‚    â”‚  - Returns      â”‚    â”‚  - IMA          â”‚
â”‚  - CDI          â”‚    â”‚  - Volatility   â”‚    â”‚  - Curves       â”‚
â”‚  - IPCA         â”‚    â”‚  - Rates        â”‚    â”‚  - Debentures   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Testes

### Executar Testes

```bash
# Executar todos os testes
pytest

# Executar com cobertura
pytest --cov=anbima_scraper

# Executar testes especÃ­ficos
pytest tests/test_indicators.py

# Executar com verbose
pytest -v
```

### Estrutura de Testes

```python
# Exemplo de teste
def test_map_indicator_selic(scraper):
    """Test SELIC indicator mapping."""
    result = scraper._map_indicator("Taxa SELIC do BC2", "DescriÃ§Ã£o")
    assert result == "selic"
```

## ğŸ”§ Desenvolvimento

### ConfiguraÃ§Ã£o do Ambiente

```bash
# Instalar pre-commit hooks
pre-commit install

# Formatar cÃ³digo
black src/ tests/

# Verificar imports
isort src/ tests/

# Verificar tipos
mypy src/

# Verificar linting
flake8 src/ tests/
```

### Adicionando Novos Scrapers

1. **Criar classe do scraper**:
```python
from ..core.scraper import BaseScraper

class NewScraper(BaseScraper):
    def __init__(self):
        super().__init__("new_scraper")
    
    def scrape(self, start_date=None, end_date=None):
        # Implementar lÃ³gica do scraper
        pass
```

2. **Adicionar configuraÃ§Ãµes**:
```python
# Em settings.py
ANBIMA_URLS["new_scraper"] = "https://anbima.com.br/new-endpoint"
FILE_PATHS["new_scraper"] = PROCESSED_DATA_DIR / "new_scraper.csv"
```

3. **Registrar no scraper principal**:
```python
# Em anbima_scraper.py
self.scrapers["new_scraper"] = NewScraper()
```

## ğŸ“ˆ Monitoramento e Logs

### NÃ­veis de Log

- **DEBUG**: InformaÃ§Ãµes detalhadas para desenvolvimento
- **INFO**: InformaÃ§Ãµes gerais sobre operaÃ§Ãµes
- **WARNING**: Avisos sobre situaÃ§Ãµes nÃ£o crÃ­ticas
- **ERROR**: Erros que impedem operaÃ§Ã£o normal

### Exemplo de Logs

```
2023-12-15 10:30:15 [INFO] anbima_scraper.core.anbima_scraper: Starting ANBIMA scraper for all data sources
2023-12-15 10:30:16 [INFO] anbima_scraper.scrapers.indicators: Scraping ANBIMA indicators
2023-12-15 10:30:17 [DEBUG] anbima_scraper.utils.http_client: Making GET request to: https://www.anbima.com.br/informacoes/indicadores/
2023-12-15 10:30:18 [INFO] anbima_scraper.utils.http_client: Request successful: 200
2023-12-15 10:30:19 [INFO] anbima_scraper.scrapers.indicators: Fetched indicators data: 25 rows
2023-12-15 10:30:20 [INFO] anbima_scraper.scrapers.indicators: Processed indicators data: 25 rows
2023-12-15 10:30:21 [INFO] anbima_scraper.scrapers.indicators: Indicators data updated successfully
```

## ğŸš¨ Troubleshooting

### Problemas Comuns

#### 1. Erro de ConexÃ£o
```
Error: Request failed: Connection timeout
```
**SoluÃ§Ã£o**: Verificar conectividade com internet e configuraÃ§Ãµes de proxy.

#### 2. Dados NÃ£o Atualizados
```
No new data to download
```
**SoluÃ§Ã£o**: Usar flag `--force` para forÃ§ar atualizaÃ§Ã£o.

#### 3. Erro de PermissÃ£o
```
Permission denied: logs/anbima_scraper.log
```
**SoluÃ§Ã£o**: Verificar permissÃµes do diretÃ³rio `logs/`.

#### 4. DependÃªncias Ausentes
```
ModuleNotFoundError: No module named 'pandas'
```
**SoluÃ§Ã£o**: Instalar dependÃªncias com `pip install -r requirements.txt`.

### Logs de Debug

Para obter mais informaÃ§Ãµes sobre problemas:

```python
import logging
logging.getLogger('anbima_scraper').setLevel(logging.DEBUG)
```

## ğŸ¤ ContribuiÃ§Ã£o

### Como Contribuir

1. **Fork** o projeto
2. **Clone** seu fork
3. **Crie** uma branch para sua feature
4. **Implemente** suas mudanÃ§as
5. **Adicione** testes
6. **Execute** os testes
7. **Commit** suas mudanÃ§as
8. **Push** para sua branch
9. **Abra** um Pull Request

### PadrÃµes de CÃ³digo

- Use **Black** para formataÃ§Ã£o
- Use **isort** para organizaÃ§Ã£o de imports
- Use **flake8** para linting
- Use **mypy** para type checking
- Adicione **docstrings** para todas as funÃ§Ãµes
- Escreva **testes** para novas funcionalidades

### Checklist de Pull Request

- [ ] CÃ³digo segue padrÃµes de formataÃ§Ã£o
- [ ] Testes passam
- [ ] DocumentaÃ§Ã£o atualizada
- [ ] Logs apropriados adicionados
- [ ] Tratamento de erros implementado

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- **ANBIMA** por disponibilizar os dados financeiros
- **Comunidade Python** pelas ferramentas e bibliotecas
- **Contribuidores** que ajudaram a melhorar o projeto

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](https://github.com/royopa/anbima_scraper/issues)
- **DocumentaÃ§Ã£o**: Este README e docstrings no cÃ³digo
- **Email**: royopa@gmail.com

---

**â­ Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela no GitHub!** 